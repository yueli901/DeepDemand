{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "# set the notebook's CWD to your repo root\n",
    "%cd D:/deepdemand\n",
    "ROOT = Path.cwd().parents[0]   # go up one level\n",
    "sys.path.insert(0, str(ROOT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Build a globally UNIQUE OD-pair pool (node-level) from all subgraph od_use.feather.\n",
    "\n",
    "- Enforces integer node IDs for O/D\n",
    "- Drops NaNs\n",
    "- Dedupes within each chunk AND against a global on-disk set (via parquet parts)\n",
    "- Writes final unique pool to a single CSV\n",
    "- Reports counts and sanity checks\n",
    "\n",
    "This is streaming: it does NOT try to hold all pairs in RAM.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "ROOT = Path(\"data/subgraphs/subgraphs\")\n",
    "OUT_DIR = Path(\"interpret/OD_pairs_pool\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# intermediate unique parts\n",
    "PART_DIR = OUT_DIR / \"parts_unique\"\n",
    "PART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FINAL_CSV = OUT_DIR / \"od_pairs_pool_unique.csv\"\n",
    "\n",
    "# how many raw rows to accumulate before flushing a part\n",
    "FLUSH_ROWS = 5_000_000\n",
    "\n",
    "def iter_od_files(root: Path):\n",
    "    # expects: data/subgraphs/subgraphs/{edge_id}/od_use.feather\n",
    "    for edge_dir in root.iterdir():\n",
    "        if not edge_dir.is_dir():\n",
    "            continue\n",
    "        f = edge_dir / \"od_use.feather\"\n",
    "        if f.is_file():\n",
    "            yield f\n",
    "\n",
    "def normalize_od_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # force numeric -> int64; strip weird float strings etc.\n",
    "    df = df[[\"O\",\"D\"]].copy()\n",
    "\n",
    "    # coerce to numeric safely\n",
    "    df[\"O\"] = pd.to_numeric(df[\"O\"], errors=\"coerce\")\n",
    "    df[\"D\"] = pd.to_numeric(df[\"D\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"O\",\"D\"])\n",
    "\n",
    "    # convert to int64 (node ids)\n",
    "    df[\"O\"] = df[\"O\"].astype(np.int64)\n",
    "    df[\"D\"] = df[\"D\"].astype(np.int64)\n",
    "\n",
    "    return df\n",
    "\n",
    "def write_part_unique(df_unique: pd.DataFrame, part_idx: int) -> Path:\n",
    "    part_path = PART_DIR / f\"part_{part_idx:05d}.parquet\"\n",
    "    df_unique.to_parquet(part_path, index=False)\n",
    "    return part_path\n",
    "\n",
    "def merge_parts_to_final_csv(part_paths, out_csv: Path):\n",
    "    # external merge: read each parquet part, concat, drop_duplicates, write csv\n",
    "    # (still can be large; if too large, do multi-pass merging)\n",
    "    print(\"[Merge] Loading parts for final dedupe...\")\n",
    "    dfs = []\n",
    "    for p in tqdm(part_paths, desc=\"Read parts\"):\n",
    "        dfs.append(pd.read_parquet(p))\n",
    "    all_df = pd.concat(dfs, ignore_index=True)\n",
    "    before = len(all_df)\n",
    "    all_df = all_df.drop_duplicates(subset=[\"O\",\"D\"], keep=\"first\")\n",
    "    after = len(all_df)\n",
    "    print(f\"[Merge] concat rows={before:,}, unique rows={after:,}\")\n",
    "\n",
    "    all_df.to_csv(out_csv, index=False)\n",
    "    print(\"Wrote:\", out_csv)\n",
    "\n",
    "def main():\n",
    "    files = list(iter_od_files(ROOT))\n",
    "    if not files:\n",
    "        raise RuntimeError(f\"No od_use.feather files found under {ROOT}\")\n",
    "\n",
    "    buf = []\n",
    "    buf_n = 0\n",
    "    part_idx = 0\n",
    "    part_paths = []\n",
    "\n",
    "    total_raw = 0\n",
    "\n",
    "    for f in tqdm(files, desc=\"Scan od_use.feather\"):\n",
    "        df = pd.read_feather(f, columns=[\"O\",\"D\"])\n",
    "        total_raw += len(df)\n",
    "        df = normalize_od_df(df)\n",
    "\n",
    "        # local dedupe first\n",
    "        df = df.drop_duplicates(subset=[\"O\",\"D\"], keep=\"first\")\n",
    "\n",
    "        buf.append(df)\n",
    "        buf_n += len(df)\n",
    "\n",
    "        if buf_n >= FLUSH_ROWS:\n",
    "            big = pd.concat(buf, ignore_index=True)\n",
    "            before = len(big)\n",
    "            big = big.drop_duplicates(subset=[\"O\",\"D\"], keep=\"first\")\n",
    "            after = len(big)\n",
    "\n",
    "            part_path = write_part_unique(big, part_idx)\n",
    "            part_paths.append(part_path)\n",
    "            print(f\"[Flush] part {part_idx} rows before={before:,} after_unique={after:,} -> {part_path.name}\")\n",
    "\n",
    "            part_idx += 1\n",
    "            buf = []\n",
    "            buf_n = 0\n",
    "\n",
    "    # final flush\n",
    "    if buf_n > 0:\n",
    "        big = pd.concat(buf, ignore_index=True)\n",
    "        before = len(big)\n",
    "        big = big.drop_duplicates(subset=[\"O\",\"D\"], keep=\"first\")\n",
    "        after = len(big)\n",
    "        part_path = write_part_unique(big, part_idx)\n",
    "        part_paths.append(part_path)\n",
    "        print(f\"[Flush] part {part_idx} rows before={before:,} after_unique={after:,} -> {part_path.name}\")\n",
    "\n",
    "    print(f\"[Stats] total_raw_rows_read={total_raw:,}\")\n",
    "    print(f\"[Stats] parts_written={len(part_paths)}\")\n",
    "\n",
    "    # final merge + global dedupe\n",
    "    merge_parts_to_final_csv(part_paths, FINAL_CSV)\n",
    "\n",
    "    # quick sanity: reload and verify uniqueness\n",
    "    df_final = pd.read_csv(FINAL_CSV)\n",
    "    dup = df_final.duplicated(subset=[\"O\",\"D\"]).sum()\n",
    "    print(f\"[Sanity] final rows={len(df_final):,} duplicated_pairs={dup}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161ccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path setting\n",
    "import sys\n",
    "from pathlib import Path\n",
    "# set the notebook's CWD to your repo root\n",
    "%cd D:/deepdemand\n",
    "ROOT = Path.cwd().parents[0]   # go up one level\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import importlib\n",
    "import interpret.od_pair_rf_shap as pipe\n",
    "importlib.reload(pipe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9787c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lv1_features(rec):\n",
    "    feats, names = [], []\n",
    "\n",
    "    # ---- population (aggregated) ----\n",
    "    pop_lv3 = rec[\"population\"][\"lv3\"]\n",
    "    v = pop_lv3[0]\n",
    "    feats.append(v)\n",
    "    names.append(\"Population\")\n",
    "\n",
    "    # ---- employment (aggregated) ----\n",
    "    emp_lv3 = rec[\"employment\"][\"lv3\"]\n",
    "    v = emp_lv3[0]\n",
    "    feats.append(v)\n",
    "    names.append(\"Employment\")\n",
    "\n",
    "    # ---- households (aggregated) ----\n",
    "    hh_lv3 = rec[\"households\"][\"lv3\"]\n",
    "    v = hh_lv3[0]\n",
    "    feats.append(v)\n",
    "    names.append(\"Households\")\n",
    "\n",
    "    # ---- area only (drop population density) ----\n",
    "    area = rec[\"area_popdensity\"][0]\n",
    "    feats.append(area)\n",
    "    names.append(\"Area\")\n",
    "\n",
    "    # ---- POI: total count ----\n",
    "    poi_vals = rec[\"poi\"]\n",
    "    feats.append(sum(poi_vals))\n",
    "    names.append(\"POIs\")\n",
    "\n",
    "    # ---- IMD: mean score ----\n",
    "    imd_vals = rec[\"imd\"]\n",
    "    feats.append(sum(imd_vals) / len(imd_vals))\n",
    "    names.append(\"Mean IMD\")\n",
    "\n",
    "    return feats, names\n",
    "\n",
    "# def extract_lv3_features(rec):\n",
    "    # feats, names = [], []\n",
    "\n",
    "    # # ---- population lv3: drop first 8, keep remaining (10) ----\n",
    "    # pop_lv3 = rec[\"population\"][\"lv3\"]\n",
    "    # for idx in range(8, len(pop_lv3)):\n",
    "    #     v = pop_lv3[idx]\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"population_lv3_{idx+1}\")\n",
    "\n",
    "    # # ---- employment lv3: drop first 21, keep remaining ----\n",
    "    # emp_lv3 = rec[\"employment\"][\"lv3\"]\n",
    "    # for idx in range(21, len(emp_lv3)):\n",
    "    #     v = emp_lv3[idx]\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"employment_lv3_{idx+1}\")\n",
    "\n",
    "    # # ---- households lv3: drop first 13, keep remaining ----\n",
    "    # hh_lv3 = rec[\"households\"][\"lv3\"]\n",
    "    # for idx in range(13, len(hh_lv3)):\n",
    "    #     v = hh_lv3[idx]\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"households_lv3_{idx+1}\")\n",
    "\n",
    "    # # ---- area_popdensity (2 dims, unchanged) ----\n",
    "    # for i, v in enumerate(rec[\"area_popdensity\"]):\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"area_popdensity_{i+1}\")\n",
    "\n",
    "    # # ---- land_use (4 dims, unchanged) ----\n",
    "    # for i, v in enumerate(rec[\"land_use\"]):\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"landuse_{i+1}\")\n",
    "\n",
    "    # # ---- poi (5 dims, unchanged) ----\n",
    "    # for i, v in enumerate(rec[\"poi\"]):\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"poi_{i+1}\")\n",
    "\n",
    "    # # ---- imd (2 dims, unchanged) ----\n",
    "    # for i, v in enumerate(rec[\"imd\"]):\n",
    "    #     feats.append(v)\n",
    "    #     names.append(f\"imd_{i+1}\")\n",
    "\n",
    "    # return feats, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193c1bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load] OD pool: interpret/OD_pairs_pool/od_pairs_pool_unique.parquet\n",
      "[Load] OD pool shape: (107317305, 2)\n",
      "[OD] Using 50,000 pairs (sampled)\n",
      "[Load] LSOA JSON: data/node_features/lsoa21_features_normalized.json\n",
      "[Load] node_to_lsoa: data/node_features/node_to_lsoa.json\n",
      "[Prep] FULL LSOA matrix via get_lsoa_vector ...\n",
      "[Prep] FULL matrix: (35672, 121)\n",
      "[PCA] Projected to 64 dims.\n",
      "[Prep] FULL->PCA: (35672, 64)\n",
      "[Prep] RF feature cache (per LSOA) ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RF cache (LSOA): 100%|██████████| 35672/35672 [00:00<00:00, 641567.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prep] Mapping node ids -> LSOA indices ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "node->lsoa: 100%|██████████| 50000/50000 [00:00<00:00, 1245117.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OD] Remaining after mapping: 50,000\n",
      "[RF] X shape: (50000, 12)\n",
      "[Load] Loaded 18 keys into MukaraLitePair\n",
      "[Score] DL half-pass scoring ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pair_score: 100%|██████████| 1/1 [00:00<00:00, 107.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Score] y stats: min=0.430641 max=1591.07 mean=4.56231 nan=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: interpret\\OD_pair_shap\\AADT_pairscore_RF_lv1\\od_pair_sample_with_xy.csv\n",
      "[RF] Using 50,000 rows after SUBSAMPLE_N=50000\n",
      "[RF] Prepared: (50000, 12) (50000,)\n",
      "\n",
      "[CACHE] Found: interpret\\OD_pair_shap\\AADT_pairscore_RF_lv1\\shap_logOD_pair_score_bar.shap_cache.npz -> skipping RF+SHAP, plotting directly.\n",
      "Saved: interpret\\OD_pair_shap\\AADT_pairscore_RF_lv1\\shap_logOD_pair_score_bar.pdf\n",
      "Saved: interpret\\OD_pair_shap\\AADT_pairscore_RF_lv1\\shap_logOD_pair_score_beeswarm.svg\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "pipe.extract_features = extract_lv1_features\n",
    "pipe.SIZE = (5,5)\n",
    "pipe.X_LABEL = \"SHAP value\"\n",
    "pipe.OUT_TAG = \"AADT_pairscore_RF_lv1\"\n",
    "pipe.OUT_DIR = pipe.Path(\"interpret/OD_pair_shap\") / pipe.OUT_TAG\n",
    "pipe.OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "pipe.OUT_SAMPLE_CSV = pipe.OUT_DIR / \"od_pair_sample_with_xy.csv\"\n",
    "pipe.OUT_BAR_PDF    = pipe.OUT_DIR / \"shap_logOD_pair_score_bar.pdf\"\n",
    "pipe.OUT_BEE_PDF    = pipe.OUT_DIR / \"shap_logOD_pair_score_beeswarm.svg\"\n",
    "pipe.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519210e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
